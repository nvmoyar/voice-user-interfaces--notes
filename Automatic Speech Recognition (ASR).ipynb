{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Automatic Speech Recognition (ASR)\n",
    "\n",
    "## Challenges in ASR\n",
    "\n",
    "1. Background noise\n",
    "2. Variability of the speaker (pitch, volume)\n",
    "3. Same word, different speeds\n",
    "4. Word Boundaries\n",
    "5. Spoken language vs written language \n",
    "\n",
    "> Words are constant, but utterances aren’t. Spectrograms of similar words pronounced by the same speaker may be more alike than Spectrograms of the same word pronounced by different speakers. \n",
    "\n",
    "[http://www.seas.upenn.edu/~cis391/Lectures/speech-rec.pdf]\n",
    "\n",
    "### Speech Recognition: Task Dimensions\n",
    "\n",
    "* Speaker Dependent, Independent, Adaptive\n",
    "            Speaker dependent: System trained for current speaker\n",
    "            Speaker independent: No modificiation per speaker\n",
    "            Speaker Adaptive: adapts an initial model to speaker\n",
    "* Read vs. dictation vs. conversational\n",
    "* Quiet Conditions vs. various noise conditions\n",
    "* Known microphone vs. unknown microphone\n",
    "* Perplexity level\n",
    "            Low perplexity: Average expected branching factor of\n",
    "        grammar < 10-20\n",
    "            High perplexity: Average expected branching factor of\n",
    "        grammar > 100 \n",
    "\n",
    "\n",
    "## Pipeline for ASR\n",
    "\n",
    "<img src=\"assets/asr-pipeline.png\" />\n",
    "\n",
    "So far, with MFCC we can extract features from speech and we can address challenges 1 and 2. This features can be turned on phonetic representation or phonemes using an acoustic model. Phonemes can be translated into words using a Lexical Decoding or Lexicon. However there are systems capable to translate the acoustic model into words. This is a **design choice** indeed, and it depends on the dimensionality of the problem. \n",
    "\n",
    "\n",
    "### The acoustic model and problems with time: same word, different speeds : intro to HMM's\n",
    "\n",
    "The problem we address here, is the common fact that the words are usually pronounced using a diferent time length. For instance, besides the different pronunciations in the word 'Hello\", it doesn't usually take the same time saying it. Further, some words might sound the same and only from the acoustic model, without providing any detail about the context, are really hard to distinguish like HEAR or HERE. \n",
    "\n",
    "Hidden Markov Models (HMM) are specially good to address the problem of variability in length, since they are really good to find patterns through time. The training set could consists of all labelled words, phonemes, sounds or groups of words, in order to determine the likelihood of a single word or phoneme. However this training becomes much more complicated when our dataset consists of utterances -phrases or whole sentences-. How could we this series of data be separated in training? Since a particular word may be connected to the previous and the next words, in order to train continuous utterances, HMM nodes are tied together as pairs which leads to an increase of dimensionality. When training words, the combinations are unbeareable, however training phonemes is a more accessible problem, since there are only 40 phonemes in English, only 1600 combinations are possible. Once the model is trained, it can be used to score new utterances. \n",
    "\n",
    "<img src=\"assets/sequence_possible_speech.png\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Adding knowledge: Language model\n",
    "\n",
    "Which combinations of words are more reasonable? Even if from phonemes we can get words, we still can't solve language ambiguities in spelling and context, since we haven't taught to the model which combinations are more likely, or at least provide knowledge about the context, to allow the model to learn from itself. The language models just does this job: inject knowledge. \n",
    "\n",
    "Every single word can be thought as a probability of distribution over many different words. Each possible sequence can be calculated as the likelihood that a particular word could have been produced by the audio signal.\n",
    "\n",
    "$$P(signal   |   w_1,w_2)$$\n",
    "\n",
    "A statistical language model does precisely that. It provides a probability distribution over sequences of words. \n",
    "\n",
    "$$word_1, word_2, ... = argmax_{w_1 w_2 ...} {P(signal | w_1,w_2,...) * P(w_1,w_2,...)}$$\n",
    "\n",
    "Even though, the dimensionality applying a statistical model is extremely huge, and some heuristics or approximate can be employed here: **It turns out that in practice, the words we speak at any time are primarily dependent upon the three to four previous words.**\n",
    "\n",
    "### N-grams\n",
    "\n",
    "N-grams are prob of single words \"I\", ordered pairs \"I love\" (bigrams), triples \"I love Science\", etc. With n-grams we can approximate the sequence probability using the chaing rule. \n",
    "\n",
    "$$ P(\"I\", \"love\", \"Science\") = P(\"I\") * P(\"love\"|\"I\") * P(\"Science\" | \"I\", \"love\") $$\n",
    "\n",
    "Then we can score these probability along with the probabilities coming from the Acoustic model to remove language ambiguities from the sequence options and **provide a better estimate of the utterance give an text**. \n",
    "\n",
    "#### Quizz: computing bigrams\n",
    "\n",
    "In the following series of quizes, you will work with 2-grams, or bigrams, as they are more commonly called. The objective is to create a function that calculates the probability that a particular sentence could occur in a corpus of text, based on the probabilities of its component bigrams. We'll do this in stages though:\n",
    "\n",
    "* Quiz 1 - Extract tokens and bigrams from a sentence\n",
    "* Quiz 2 - Calculate probabilities for bigrams\n",
    "* Quiz 3 - Calculate the log probability of a given sentence based on a corpus of text using bigrams\n",
    "\n",
    "##### Assumptions and terminology\n",
    "\n",
    "* Utterance : 'I love language models'\n",
    "* Tokens (word list from utterance + start tag + ending tag): \n",
    "\n",
    "$$ tokens = [\"<s>\", \"I\", \"love\", \"language\", \"models\", \"</s>\"]$$\n",
    "\n",
    "\n",
    "* Bigrams: The bigrams for this sentence are represented as a list of lower case ordered pairs of tokens:\n",
    "\n",
    "\n",
    "$$bigrams=[[(\"<s>\", \"i\"), (\"i\", \"love\"), (\"love\", \"language\"), (\"language\", \"models\"), (\"models\", \"</s>\")]]$$\n",
    "\n",
    "\n",
    "##### Quiz 1 Instructions\n",
    "\n",
    "In the quiz below, write a function that returns a list of tokens and a list of bigrams for a given sentence. You will need to first break a sentence into words in a list, then add a token to the start and end of the list to represent the start and end of the sentence.\n",
    "\n",
    "Your final lists should be in the format shown above and called out in the function doc string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['<s>', 'the', 'old', 'man', 'spoke', 'to', 'me', '</s>'],\n",
       "  [('<s>', 'the'),\n",
       "   ('the', 'old'),\n",
       "   ('old', 'man'),\n",
       "   ('man', 'spoke'),\n",
       "   ('spoke', 'to'),\n",
       "   ('to', 'me'),\n",
       "   ('me', '</s>')]),\n",
       " (['<s>', 'me', 'to', 'spoke', 'man', 'old', 'the', '</s>'],\n",
       "  [('<s>', 'me'),\n",
       "   ('me', 'to'),\n",
       "   ('to', 'spoke'),\n",
       "   ('spoke', 'man'),\n",
       "   ('man', 'old'),\n",
       "   ('old', 'the'),\n",
       "   ('the', '</s>')]),\n",
       " (['<s>', 'old', 'man', 'me', 'old', 'man', 'me', '</s>'],\n",
       "  [('<s>', 'old'),\n",
       "   ('old', 'man'),\n",
       "   ('man', 'me'),\n",
       "   ('me', 'old'),\n",
       "   ('old', 'man'),\n",
       "   ('man', 'me'),\n",
       "   ('me', '</s>')])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "\n",
    "def get_token(sentence): \n",
    "    token_list = sentence.split(\" \")\n",
    "    token_list.insert(0, \"<s>\")\n",
    "    token_list.append(\"</s>\")   \n",
    "    return token_list\n",
    "\n",
    "\n",
    "def get_bigram(token_list): \n",
    "    \n",
    "    pairs=[]  \n",
    "    [pairs.append(token_list[i]) for i in range(1, len(token_list))]       \n",
    "    return list(zip(token_list, pairs))\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = get_token(sentence)\n",
    "    sentence_bigrams = get_bigram(sentence_tokens)\n",
    "    \n",
    "    return sentence_tokens, sentence_bigrams\n",
    "\n",
    "[sentence_to_bigrams(sentence) for sentence in test_sentences] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Probabilities and Likelihoods with Bigrams\n",
    "\n",
    "The probability of a series of words can be calculated from the chained probabilities of its history:\n",
    "\n",
    "$$  P_{w_1, w_2, ...,w_n} = \\prod_{i=1}^{n} P(w_i| w_1 w_2,...,w_{i-1})$$\n",
    "\n",
    "The probabilities of sequence occurrences in a large textual corpus can be calculated this way and used as a **language model to add grammar and contextual knowledge to a speech recognition system**. However, there is a prohibitively large number of calculations for all the possible sequences of varying length in a large textual corpus.\n",
    "\n",
    "To address this problem, we use the Markov Assumption to approximate a sequence probability with a shorter sequence.\n",
    "\n",
    "###### Markov Assumption\n",
    "\n",
    "In probability theory and statistics, the term Markov property refers to the memoryless property of a stochastic process. It is named after the Russian mathematician Andrey Markov.\n",
    "\n",
    "**A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it**. A process with this property is called a Markov process. The term strong Markov property is similar to the Markov property, except that the meaning of \"present\" is defined in terms of a random variable known as a stopping time. Ex. Particles Brownian motion for times 0 ≤ t ≤ 2. Brownian motion has the Markov property, as the displacement of the particle does not depend on its past displacements.\n",
    "\n",
    "The term Markov assumption is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model.\n",
    "\n",
    "A Markov random field extends this property to two or more dimensions or to random variables defined for an interconnected network of items. An example of a model for such a field is the Ising model. A discrete-time stochastic process satisfying the Markov property is known as a Markov chain.\n",
    "\n",
    "Thus, we could approximate\n",
    "\n",
    "$$  P_{w_1, w_2, ...,w_n} \\approx \\prod_{i=1}^{n} P(w_i| w_{i-k}...w_{i-1})$$\n",
    "\n",
    "\n",
    "We can calculate the probabilities by using counts of the bigrams (numerator) and individual tokens (denominator). The counts are represented below with the c() operator:\n",
    "\n",
    "$$  P_{w_i|w_{i-1}} = \\dfrac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$\n",
    "\n",
    "In the quiz below, write a function that returns a probability dictionary when given a lists of tokens and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bigrams_from_transcript(filename):\n",
    "    \"\"\"\n",
    "    read a file of sentences, adding start '<s>' and stop '</s>' tags; Tokenize it into a list of lower case words\n",
    "    and bigrams\n",
    "    :param filename: string \n",
    "        filename: path to a text file consisting of lines of non-puncuated text; assume one sentence per line\n",
    "    :return: list, list\n",
    "        tokens: ordered list of words found in the file\n",
    "        bigrams: a list of ordered two-word tuples found in the file\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    bigrams = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line_tokens, line_bigrams = sentence_to_bigrams(line)\n",
    "            tokens = tokens + line_tokens\n",
    "            bigrams = bigrams + line_bigrams\n",
    "    return tokens, bigrams\n",
    "\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    sentence_bigrams = []\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))\n",
    "    return sentence_tokens, sentence_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def sample_run():\n",
    "    # sample usage by test code (this definition not actually run for the quiz)\n",
    "    tokens, bigrams = bigrams_from_transcript('assets/transcript.txt')\n",
    "    bg_dict = bigram_mle(tokens, bigrams)\n",
    "    print(bg_dict)\n",
    "\n",
    "\n",
    "def bigram_mle(tokens, bigrams):\n",
    "    \"\"\"\n",
    "    provide a dictionary of probabilities for all bigrams in a corpus of text\n",
    "    the calculation is based on maximum likelihood estimation and does not include\n",
    "    any smoothing.  A tag '<unk>' has been added for unknown probabilities.\n",
    "    :param tokens: list\n",
    "        tokens: list of all tokens in the corpus\n",
    "    :param bigrams: list\n",
    "        bigrams: list of all two word tuples in the corpus\n",
    "    :return: dict\n",
    "        bg_mle_dict: a dictionary of bigrams:\n",
    "            key: tuple of two bigram words, in order OR <unk> key\n",
    "            value: float probability\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    bg_mle_dict = {}\n",
    "    bg_mle_dict['<unk>'] = 0.\n",
    "   \n",
    "    # 530 words in the transcript   \n",
    "    bigram_raw_counts = Counter(bigrams) # len = 515, Counter({('<s>', 'the'): 5, ('do', 'you'): 5...| c(wi−1,wi)\n",
    "    token_raw_counts = Counter(tokens)   # len = 276, Counter({'the': 40, '</s>': 29, 'you': 17.... |  c(wi−1)    \n",
    "    \n",
    "    for bg in bigram_raw_counts: # for every possible bigram\n",
    "        bg_mle_dict[bg] = bigram_raw_counts[bg] / token_raw_counts[bg[0]]\n",
    "    return bg_mle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('escape', 'him'): 0.5, ('this', 'gentleman'): 0.16666666666666666, ('no', '</s>'): 0.5, ('therefore', 'entered'): 1.0, ('going', 'sir'): 1.0, ('towards', 'those'): 0.25, ('rave', 'sir'): 0.5, ('throat', 'swelled'): 1.0, ('honor', 'that'): 1.0, ('complete', 'this'): 1.0, ('as', 'the'): 0.125, ('do', 'yet'): 0.125, ('courage', 'to'): 1.0, ('asked', 'morrel'): 1.0, ('attracted', 'towards'): 1.0, ('say', 'that'): 1.0, ('<s>', 'noirtier'): 0.06896551724137931, ('kind', 'i'): 1.0, ('the', 'sight'): 0.025, ('go', 'do'): 0.5, ('swelled', 'his'): 1.0, ('near', 'the'): 1.0, ('which', 'became'): 0.2, ('remain', 'buried'): 1.0, ('<s>', 'at'): 0.034482758620689655, ('doctor', 'approached'): 0.5, ('we', 'may'): 0.5, ('davrigny', 'rushed'): 0.3333333333333333, ('his', 'pores'): 0.14285714285714285, ('unclosed', 'the'): 1.0, ('in', 'his'): 0.1111111111111111, ('a', 'powerful'): 0.14285714285714285, ('the', 'silent'): 0.025, ('into', 'my'): 1.0, ('noirtier', 'looked'): 0.25, ('to', 'you'): 0.07692307692307693, ('sobs', 'exclamations'): 1.0, ('the', 'whole'): 0.025, ('to', 'spend'): 0.07692307692307693, ('this', 'horrible'): 0.16666666666666666, ('the', 'door'): 0.05, ('seeking', 'any'): 1.0, ('which', 'draws'): 0.2, ('pores', 'if'): 1.0, ('<s>', 'it'): 0.034482758620689655, ('exclaimed', 'villefort'): 1.0, ('father', '</s>'): 0.3333333333333333, ('struck', 'with'): 1.0, ('is', 'a'): 0.5, ('so', 'that'): 0.3333333333333333, ('escape', 'the'): 0.5, ('room', 'do'): 0.5, ('keep', 'this'): 1.0, ('then', 'lifted'): 1.0, ('noirtier', 'was'): 0.25, ('a', 'man'): 0.14285714285714285, ('tears', 'silently'): 1.0, ('but', 'the'): 0.2, ('the', 'affirmative'): 0.025, ('<s>', 'for'): 0.034482758620689655, ('please', 'you'): 1.0, ('but', 'sobs'): 0.2, ('him', 'inhale'): 0.16666666666666666, ('assassin', 'asked'): 1.0, ('placed', 'over'): 1.0, ('amongst', 'the'): 0.5, ('the', 'old'): 0.1, ('a', 'sign'): 0.14285714285714285, ('cry', 'issued'): 0.3333333333333333, ('inhale', 'a'): 1.0, ('cheeks', '</s>'): 0.5, ('made', 'valentine'): 0.3333333333333333, ('nothing', 'was'): 1.0, ('an', 'exclamation'): 0.5, ('i', 'beseech'): 0.1111111111111111, ('is', 'the'): 0.5, ('you', 'know'): 0.058823529411764705, ('rushed', 'towards'): 1.0, ('than', 'mine'): 0.5, ('this', 'secret'): 0.16666666666666666, ('sign', 'in'): 1.0, ('bloodshot', 'the'): 1.0, ('utterance', 'of'): 1.0, ('mine', '</s>'): 1.0, ('again', 'visit'): 1.0, ('the', 'cry'): 0.025, ('extraordinary', 'weight'): 1.0, ('and', 'thus'): 0.07692307692307693, ('leave', 'alone'): 1.0, ('good', 'italian'): 1.0, ('accompany', 'this'): 1.0, ('melancholy', 'smiles'): 1.0, ('will', 'oblige'): 0.2, ('soul', 'of'): 1.0, ('a', 'cry'): 0.2857142857142857, ('he', 'said'): 0.125, ('whole', 'soul'): 1.0, ('see', 'him'): 1.0, ('noirtier', '</s>'): 0.25, ('spend', 'half'): 1.0, ('us', 'towards'): 1.0, ('go', 'in'): 0.5, ('the', 'culprits'): 0.025, ('old', 'mans'): 0.25, ('you', 'do'): 0.058823529411764705, ('down', 'his'): 1.0, ('the', 'throat'): 0.025, ('this', 'touching'): 0.16666666666666666, ('the', 'courage'): 0.025, ('mean', 'sir'): 1.0, ('time', 'nothing'): 0.5, ('groaned', 'beneath'): 1.0, ('man', 'accustomed'): 0.2, ('culprits', 'name'): 1.0, ('more', 'fervent'): 1.0, ('a', 'good'): 0.14285714285714285, ('but', 'he'): 0.2, ('thus', 'speak'): 0.5, ('young', 'man'): 1.0, ('villefort', 'be'): 0.3333333333333333, ('me', 'will'): 0.3333333333333333, ('will', 'bring'): 0.2, ('to', 'say'): 0.07692307692307693, ('from', 'his'): 1.0, ('will', 'you'): 0.2, ('district', 'doctor'): 1.0, ('with', 'the'): 0.25, ('thirsts', 'for'): 1.0, ('and', 'made'): 0.07692307692307693, ('i', 'rave'): 0.1111111111111111, ('visit', 'the'): 1.0, ('with', 'you'): 0.25, ('sheet', 'which'): 1.0, ('must', 'i'): 1.0, ('chamber', '</s>'): 0.5, ('has', 'revealed'): 1.0, ('be', 'alone'): 0.3333333333333333, ('can', 'go'): 0.5, ('in', 'which'): 0.1111111111111111, ('introducing', 'him'): 1.0, ('in', 'its'): 0.1111111111111111, ('in', 'and'): 0.1111111111111111, ('him', 'as'): 0.16666666666666666, ('away', 'and'): 1.0, ('landing', 'he'): 1.0, ('this', 'moment'): 0.16666666666666666, ('some', 'time'): 1.0, ('the', 'landing'): 0.025, ('next', 'door'): 1.0, ('seemed', 'centred'): 1.0, ('any', 'further'): 1.0, ('magnetism', 'which'): 1.0, ('silent', 'agony'): 0.5, ('door', 'so'): 0.3333333333333333, ('<s>', 'morrel'): 0.034482758620689655, ('cry', '</s>'): 0.3333333333333333, ('his', 'attention'): 0.14285714285714285, ('beneath', 'an'): 1.0, ('attention', '</s>'): 1.0, ('do', 'not'): 0.125, ('he', 'had'): 0.125, ('approached', 'with'): 1.0, ('<s>', 'the'): 0.1724137931034483, ('but', 'in'): 0.2, ('revenge', 'as'): 1.0, ('prayers', '</s>'): 0.5, ('half', 'his'): 1.0, ('he', 'was'): 0.25, ('morrel', 'yes'): 0.2, ('the', 'young'): 0.025, ('he', 'stopped'): 0.125, ('his', 'hand'): 0.14285714285714285, ('the', 'mute'): 0.025, ('<s>', 'do'): 0.034482758620689655, ('issued', 'from'): 1.0, ('corpse', '</s>'): 1.0, ('by', 'introducing'): 0.5, ('temples', 'became'): 1.0, ('we', 'mourn'): 0.5, ('the', 'irresistible'): 0.025, ('can', 'he'): 0.5, ('explanation', 'and'): 1.0, ('of', 'honor'): 0.1, ('of', 'a'): 0.2, ('sir', 'exclaimed'): 0.3333333333333333, ('alone', '</s>'): 0.3333333333333333, ('morrel', '</s>'): 0.2, ('sir', 'and'): 0.3333333333333333, ('hand', 'towards'): 1.0, ('that', 'you'): 0.25, ('that', 'this'): 0.25, ('he', 'understand'): 0.125, ('not', 'the'): 0.25, ('dead', 'he'): 1.0, ('of', 'the'): 0.3, ('mans', 'eyes'): 1.0, ('you', 'not'): 0.11764705882352941, ('who', 'lives'): 0.5, ('hoarse', 'voice'): 1.0, ('call', 'on'): 1.0, ('touching', 'emotion'): 1.0, ('said', 'the'): 0.25, ('said', 'in'): 0.25, ('centred', 'in'): 1.0, ('voice', 'give'): 1.0, ('terrible', 'to'): 1.0, ('the', 'face'): 0.025, ('father', 'thirsts'): 0.3333333333333333, ('the', 'bed'): 0.025, ('to', 'witness'): 0.07692307692307693, ('i', 'am'): 0.1111111111111111, ('even', 'he'): 1.0, ('was', 'heard'): 0.14285714285714285, ('so', 'often'): 0.3333333333333333, ('of', 'horror'): 0.1, ('have', 'loved'): 1.0, ('i', 'do'): 0.2222222222222222, ('and', 'the'): 0.07692307692307693, ('though', 'he'): 1.0, ('suffered', 'an'): 1.0, ('alone', 'no'): 0.3333333333333333, ('epilepsy', 'nothing'): 1.0, ('will', 'excuse'): 0.2, ('wish', 'to'): 1.0, ('my', 'childs'): 0.3333333333333333, ('his', 'cheeks'): 0.2857142857142857, ('was', 'placed'): 0.14285714285714285, ('silence', '</s>'): 1.0, ('and', 'just'): 0.07692307692307693, ('said', 'villefort'): 0.25, ('fixed', 'on'): 0.5, ('two', 'men'): 0.5, ('witness', 'the'): 1.0, ('irresistible', 'magnetism'): 1.0, ('was', 'struck'): 0.14285714285714285, ('you', 'hear'): 0.058823529411764705, ('man', 'seemed'): 0.2, ('him', 'into'): 0.16666666666666666, ('endeavoring', 'to'): 1.0, ('shall', 'i'): 0.5, ('sight', 'of'): 1.0, ('ourselves', 'the'): 1.0, ('revealed', 'the'): 1.0, ('to', 'keep'): 0.07692307692307693, ('do', 'you'): 0.625, ('gentlemen', 'he'): 1.0, ('nearest', 'said'): 1.0, ('fervent', 'than'): 1.0, ('what', 'do'): 1.0, ('pale', 'motionless'): 1.0, ('two', 'doctors'): 0.5, ('yes', 'replied'): 0.3333333333333333, ('out', 'as'): 1.0, ('powerful', 'restorative'): 1.0, ('you', 'please'): 0.058823529411764705, ('purple', 'as'): 1.0, ('despair', 'of'): 1.0, ('man', 'made'): 0.2, ('mute', 'despair'): 1.0, ('silently', 'rolled'): 1.0, ('indifference', 'of'): 1.0, ('upon', 'morrel'): 1.0, ('entered', 'the'): 1.0, ('the', 'district'): 0.05, ('which', 'was'): 0.2, ('word', 'of'): 1.0, ('those', 'who'): 0.5, ('morrel', 'with'): 0.2, ('secret', 'shall'): 0.5, ('and', 'villefort'): 0.07692307692307693, ('you', 'wish'): 0.058823529411764705, ('<s>', 'my'): 0.034482758620689655, ('staircase', 'groaned'): 1.0, ('the', 'two'): 0.05, ('whom', 'we'): 1.0, ('in', 'the'): 0.1111111111111111, ('was', 'something'): 0.14285714285714285, ('that', 'chamber'): 0.25, ('cry', 'frightful'): 0.3333333333333333, ('my', 'father'): 0.6666666666666666, ('villefort', 'in'): 0.3333333333333333, ('oblige', 'me'): 1.0, ('you', 'yes'): 0.058823529411764705, ('death', 'chamber'): 1.0, ('shall', 'forever'): 0.5, ('one', 'of'): 1.0, ('the', 'indifference'): 0.025, ('in', 'that'): 0.1111111111111111, ('noirtier', 'whose'): 0.25, ('horrible', 'secret'): 1.0, ('as', 'you'): 0.25, ('agony', 'the'): 1.0, ('motionless', 'and'): 1.0, ('yes', '</s>'): 0.6666666666666666, ('villefort', 'without'): 0.3333333333333333, ('remained', 'fixed'): 1.0, ('weight', '</s>'): 1.0, ('italian', 'abbe'): 1.0, ('which', 'he'): 0.2, ('<s>', 'said'): 0.034482758620689655, ('yet', 'even'): 1.0, ('it', 'was'): 1.0, ('exclamations', 'and'): 1.0, ('restorative', '</s>'): 1.0, ('and', 'silent'): 0.07692307692307693, ('i', 'only'): 0.1111111111111111, ('to', 'accompany'): 0.07692307692307693, ('as', 'i'): 0.25, ('to', 'bear'): 0.07692307692307693, ('taken', 'i'): 1.0, ('<s>', 'oh'): 0.034482758620689655, ('surprise', 'to'): 1.0, ('doctor', 'is'): 0.5, ('me', 'by'): 0.3333333333333333, ('and', 'i'): 0.07692307692307693, ('i', 'leave'): 0.1111111111111111, ('stopped', 'on'): 1.0, ('unable', 'to'): 1.0, ('him', 'by'): 0.16666666666666666, ('you', 'rave'): 0.058823529411764705, ('minutes', 'the'): 1.0, ('sadly', 'yes'): 1.0, ('wanting', 'to'): 1.0, ('<s>', 'what'): 0.034482758620689655, ('fixed', 'his'): 0.5, ('your', 'word'): 1.0, ('do', 'to'): 0.125, ('him', '</s>'): 0.3333333333333333, ('of', 'this'): 0.1, ('<s>', 'gentlemen'): 0.034482758620689655, ('to', 'escape'): 0.15384615384615385, ('here', 'is'): 1.0, ('the', 'veins'): 0.025, ('often', 'made'): 1.0, ('me', 'your'): 0.3333333333333333, ('those', 'melancholy'): 0.5, ('turned', 'away'): 1.0, ('pass', '</s>'): 1.0, ('in', 'a'): 0.1111111111111111, ('you', 'can'): 0.058823529411764705, ('chamber', 'but'): 0.5, ('back', '</s>'): 1.0, ('father', 'has'): 0.3333333333333333, ('the', 'nearest'): 0.025, ('became', 'purple'): 0.5, ('he', 'then'): 0.125, ('made', 'him'): 0.3333333333333333, ('with', 'one'): 0.25, ('<s>', 'must'): 0.034482758620689655, ('the', 'key'): 0.025, ('davrigny', 'unable'): 0.3333333333333333, ('i', 'pass'): 0.1111111111111111, ('valentine', 'happy'): 1.0, ('the', 'assassin'): 0.025, ('the', 'death'): 0.025, ('but', 'can'): 0.2, ('you', 'mean'): 0.058823529411764705, ('be', 'more'): 0.3333333333333333, ('door', 'to'): 0.3333333333333333, ('with', 'epilepsy'): 0.25, ('his', 'eyes'): 0.14285714285714285, ('to', 'again'): 0.07692307692307693, ('for', 'whom'): 0.3333333333333333, ('veins', 'of'): 1.0, ('the', 'corpse'): 0.025, ('net', 'in'): 1.0, ('morrel', 'suffered'): 0.2, ('and', 'surprise'): 0.07692307692307693, ('to', 'see'): 0.07692307692307693, ('of', 'those'): 0.1, ('if', 'we'): 1.0, ('the', 'utterance'): 0.025, ('oh', 'you'): 1.0, ('whose', 'tears'): 1.0, ('childs', 'room'): 1.0, ('<s>', 'and'): 0.034482758620689655, ('at', 'this'): 1.0, ('abbe', 'who'): 1.0, ('smiles', 'which'): 1.0, ('accustomed', 'to'): 1.0, ('the', 'net'): 0.025, ('without', 'seeking'): 1.0, ('the', 'lips'): 0.025, ('something', 'terrible'): 1.0, ('vain', 'endeavoring'): 1.0, ('davrigny', 'said'): 0.3333333333333333, ('an', 'extraordinary'): 0.5, ('heard', 'in'): 1.0, ('and', 'out'): 0.07692307692307693, ('frightful', 'in'): 1.0, ('which', 'had'): 0.2, ('was', 'near'): 0.14285714285714285, ('hear', '</s>'): 1.0, ('excuse', 'me'): 1.0, ('said', 'morrel'): 0.25, ('hesitate', 'to'): 1.0, ('lifted', 'the'): 1.0, ('affirmative', '</s>'): 1.0, ('morrel', 'sadly'): 0.2, ('prayers', 'will'): 0.5, ('and', 'will'): 0.07692307692307693, ('the', 'room'): 0.025, ('speak', 'a'): 1.0, ('<s>', 'go'): 0.034482758620689655, ('in', 'vain'): 0.1111111111111111, ('give', 'me'): 1.0, ('thus', 'fixed'): 0.5, ('who', 'have'): 0.5, ('old', 'man'): 0.75, ('lips', '</s>'): 1.0, ('not', 'father'): 0.25, ('than', 'five'): 0.5, ('silent', 'as'): 0.5, ('room', 'alone'): 0.5, ('gentleman', 'here'): 1.0, ('bear', 'the'): 1.0, ('replied', 'noirtier'): 1.0, ('you', 'as'): 0.11764705882352941, ('secret', 'do'): 0.5, ('just', 'unclosed'): 1.0, ('had', 'so'): 0.5, ('a', 'hoarse'): 0.14285714285714285, ('further', 'explanation'): 1.0, ('drew', 'back'): 1.0, ('the', 'staircase'): 0.025, ('by', 'the'): 0.5, ('you', 'will'): 0.11764705882352941, ('am', 'going'): 1.0, ('loved', 'the'): 1.0, ('towards', 'the'): 0.5, ('name', 'my'): 1.0, ('<s>', 'davrigny'): 0.10344827586206896, ('as', 'to'): 0.125, ('key', 'of'): 1.0, ('as', 'much'): 0.125, ('eyes', 'remained'): 0.5, ('was', 'wanting'): 0.14285714285714285, ('be', 'so'): 0.3333333333333333, ('he', 'conjures'): 0.125, ('bed', 'pale'): 1.0, ('five', 'minutes'): 1.0, ('priest', 'with'): 1.0, ('understand', 'you'): 1.0, ('time', 'amongst'): 0.5, ('<s>', 'but'): 0.10344827586206896, ('became', 'bloodshot'): 0.5, ('people', 'for'): 1.0, ('not', 'hesitate'): 0.25, ('know', 'the'): 1.0, ('and', 'temples'): 0.07692307692307693, ('buried', 'amongst'): 1.0, ('amongst', 'ourselves'): 0.5, ('that', 'no'): 0.25, ('less', 'than'): 1.0, ('its', 'silence'): 1.0, ('over', 'the'): 1.0, ('draws', 'us'): 1.0, ('alone', 'you'): 0.3333333333333333, ('the', 'sheet'): 0.025, ('door', '</s>'): 0.3333333333333333, ('made', 'a'): 0.3333333333333333, ('forever', 'remain'): 1.0, ('moment', 'the'): 1.0, ('only', 'wish'): 1.0, ('for', 'revenge'): 0.3333333333333333, ('had', 'not'): 0.5, ('so', 'kind'): 0.3333333333333333, ('conjures', 'you'): 1.0, ('the', 'people'): 0.025, ('man', 'and'): 0.2, ('i', 'call'): 0.1111111111111111, ('eyes', 'which'): 0.5, ('the', 'priest'): 0.025, ('not', '</s>'): 0.25, ('towards', 'him'): 0.25, ('rolled', 'down'): 1.0, ('on', 'the'): 0.6666666666666666, ('<s>', 'i'): 0.06896551724137931, ('on', 'him'): 0.3333333333333333, ('you', 'shall'): 0.058823529411764705, ('happy', 'and'): 1.0, ('mourn', 'extended'): 1.0, ('bring', 'the'): 1.0, ('his', 'time'): 0.14285714285714285, ('lives', 'next'): 1.0, ('exclamation', 'of'): 1.0, ('will', 'be'): 0.2, ('extended', 'his'): 1.0, ('may', 'thus'): 1.0, ('rave', '</s>'): 0.5, ('was', 'taken'): 0.14285714285714285, ('emotion', 'turned'): 1.0, ('beseech', 'you'): 1.0, ('the', 'dead'): 0.025, ('to', 'complete'): 0.07692307692307693, ('no', 'prayers'): 0.5, ('doctors', 'therefore'): 1.0, ('this', 'but'): 0.16666666666666666, ('of', 'noirtier'): 0.1, ('to', 'be'): 0.07692307692307693, ('and', 'attracted'): 0.07692307692307693, ('for', 'some'): 0.3333333333333333, ('much', 'as'): 1.0, '<unk>': 0.0, ('horror', 'and'): 1.0, ('men', 'drew'): 1.0, ('sir', '</s>'): 0.3333333333333333, ('as', 'though'): 0.125, ('in', 'less'): 0.1111111111111111, ('and', 'prayers'): 0.07692307692307693, ('<s>', 'asked'): 0.034482758620689655, ('face', 'and'): 1.0, ('you', 'and'): 0.058823529411764705, ('looked', 'upon'): 1.0, ('cheeks', 'and'): 0.5, ('man', '</s>'): 0.2}\n"
     ]
    }
   ],
   "source": [
    "sample_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Smoothing and logs\n",
    "\n",
    "There are still a couple of problems to sort out before we use the bigram probability dictionary to calculate the probabilities of new sentences:\n",
    "\n",
    "\n",
    " 1- **Some possible combinations may not exist in our probability dictionary but are still possible**. We don't want to multiply in a probability of 0 just because our original corpus was deficient. This is solved through \"smoothing\" \n",
    "-a technique used to smooth categorical data-. There are a number of methods for this, but a simple one is the Laplace smoothing with the \"add-one\" estimate where V is the size of the vocabulary for the corpus, i.e. the number of unique tokens:\n",
    "\n",
    "\n",
    "$$  P_{add1(w_i|w_{i-1})} = \\dfrac{c(w_{i-1}, w_i) + 1}{c(w_{i-1}) + V}$$\n",
    "\n",
    "\n",
    " 2- Repeated multiplications of small probabilities can cause underflow problems in computers when the values become too small. To solve this, we will calculate all probabilities in log space:\n",
    "\n",
    "$$ log(p_1 * p_2 * p_3 * p_4) = logp_1 + logp_2 + logp_3 + logp_4$$\n",
    "\n",
    "\n",
    "##### Quiz 3 Instructions\n",
    "\n",
    "Write a function that calculates the log probability for a given sentence, using this log probability dictionary. If all goes well, you should observe that more likely sentences yield higher values for the log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def bigrams_from_transcript(filename):\n",
    "    \"\"\"\n",
    "    read a file of sentences, adding start '<s>' and stop '</s>' tags; Tokenize it into a list of lower case words\n",
    "    and bigrams\n",
    "    :param filename: string \n",
    "        filename: path to a text file consisting of lines of non-puncuated text; assume one sentence per line\n",
    "    :return: list, list\n",
    "        tokens: ordered list of words found in the file\n",
    "        bigrams: a list of ordered two-word tuples found in the file\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    bigrams = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line_tokens, line_bigrams = sentence_to_bigrams(line)\n",
    "            tokens = tokens + line_tokens\n",
    "            bigrams = bigrams + line_bigrams\n",
    "    return tokens, bigrams\n",
    "\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    sentence_bigrams = []\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))\n",
    "    return sentence_tokens, sentence_bigrams\n",
    "\n",
    "def bigram_add1_logs(transcript_file):\n",
    "    \"\"\"\n",
    "    provide a smoothed log probability dictionary based on a transcript\n",
    "    :param transcript_file: string\n",
    "        transcript_file is the path filename containing unpunctuated text sentences\n",
    "    :return: dict\n",
    "        bg_add1_log_dict: dictionary of smoothed bigrams log probabilities including\n",
    "        tags: <s>: start of sentence, </s>: end of sentence, <unk>: unknown placeholder probability\n",
    "    \"\"\"\n",
    "\n",
    "    tokens, bigrams = bigrams_from_transcript(transcript_file)\n",
    "    token_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    vocab_count = len(token_counts)\n",
    "\n",
    "    bg_addone_dict = {}\n",
    "    for bg in bigram_counts:\n",
    "        bg_addone_dict[bg] = np.log((bigram_counts[bg] + 1.) / (token_counts[bg[0]] + vocab_count)) # Laplace Smoothing\n",
    "    bg_addone_dict['<unk>'] = np.log(1. / vocab_count)\n",
    "    return bg_addone_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "def sample_run():\n",
    "    # sample usage by test code (this definition not actually run for the quiz)\n",
    "    bigram_log_dict = bigram_add1_logs('assets/transcript.txt')\n",
    "    for sentence in test_sentences:\n",
    "        print('*** \"{}\"'.format(sentence))\n",
    "        print(log_prob_of_sentence(sentence, bigram_log_dict))\n",
    "\n",
    "def log_prob_of_sentence(sentence, bigram_log_dict):\n",
    "    # get the sentence bigrams\n",
    "    s_tokens, s_bigrams = sentence_to_bigrams(sentence)\n",
    "\n",
    "    # add the log probabilites of the bigrams in the sentence\n",
    "    total_log_prob = 0.\n",
    "    \n",
    "    for bg in s_bigrams:\n",
    "        if bg in bigram_log_dict:\n",
    "            total_log_prob = total_log_prob + bigram_log_dict[bg]\n",
    "        else:\n",
    "            total_log_prob = total_log_prob + bigram_log_dict['<unk>']\n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** \"the old man spoke to me\"\n",
      "-34.8049553135\n",
      "*** \"me to spoke man old the\"\n",
      "-39.34280606\n",
      "*** \"old man me old man me\"\n",
      "-36.5989948127\n"
     ]
    }
   ],
   "source": [
    "sample_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Unfolded view of ASR Architecture\n",
    "\n",
    "<img src=\"assets/asr-arch.png\" width=\"500\"/>\n",
    "\n",
    "[https://web.stanford.edu/class/cs224s/lectures/224s.17.lec3.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Hidden Markov Models (HMM)\n",
    "\n",
    "** Could we use frequency counts from the text to help compute the probability that the next letter in sequence would be a vowel? ** The HMM is a sequence model. A sequence model or sequence classifier is a model whose job is to assign a label or class to each unit in a sequence, thus mapping a sequence of observations to a sequence of labels. \n",
    "\n",
    "An HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences,\n",
    "whatever), they compute a probability distribution over possible sequences of labels and choose the best label sequence. Hidden Markov models (HMMs) are a way of relating a sequence of observations to a sequence of hidden classes or hidden states that explain the observations. \n",
    "\n",
    "In this part we present the mathematics of the HMM, beginning with the **Markov chain and then including the main three constituent algorithms: **\n",
    "\n",
    "* the **Viterbi algorithm**. The process of discovering the sequence of hidden states, given the sequence of observations, is known as decoding or inference. The Viterbi algorithm is commonly used for decoding.\n",
    "\n",
    "* the **Forward algorithm and the Baum-Welch or EM algorithm for unsupervised** (or semi-supervised) learning. The parameters of an HMM are the A transition probability matrix and the B observation likelihood matrix. Both can be trained with the Baum-Welch or forward-backward algorithm.\n",
    "\n",
    "### Markov Chain = First-order observable Markov Model\n",
    "\n",
    "First, let’s be more formal and view a Markov chain as a kind of probabilistic graphical model: a way of representing probabilistic assumptions in a graph. **A Markov chain is specified by the following components and embodies an important assumption about these probabilities: **\n",
    "\n",
    "* A **set of N states** described by $Q$: $ Q = q_1, q_2…q_N$ the state at time $t$ is $q_t$\n",
    "\n",
    "\n",
    "* A **transition probability matrix A**:\n",
    "  * A set of probabilities $$A = a_{01}a_{02}…a_{n1}…a_{nn}$$\n",
    "  * Each $a_{ij}$ represents the probability of transitioning from state i to state j\n",
    "  * The\tset\tof\tthese\tis\tthe\ttransition\tprobability\tmatrix\tA \n",
    "  \n",
    "  $$a_{ij} = P(q_t = j | q_{t−1} = i) 1≤ i, j ≤ N $$\n",
    "  $$\\sum_{j=1}^n a_{ij} = 1 ∀i$$\n",
    "  \n",
    "* Distinguished\tstart and end states. The start state may be a state but it can also be represented by a vector $\\pi$ An initial distribution over probability of start states. If it so, the sum of all N $\\pi$ start states = 1.   \n",
    "\n",
    "<img src=\"assets/hmm.png\" width=\"500\" />\n",
    "\n",
    "#### Markov's chain assumption\n",
    "\n",
    "In a first-order Markov chain, **the probability of a particular state depends only on the previous state:**\n",
    "\n",
    "  $$ P(q_i|q_1...q_{i−1}) = P(q_i|q_{i−1})$$\n",
    "    \n",
    "    \n",
    "### Hidden Markov Model \n",
    "\n",
    "A Markov chain is useful when we need to compute a probability for a sequence of events that we can observe in the world. However, in many cases the events we are interested in, may not be directly observable in the world: \n",
    "\n",
    "\n",
    "To explain by example, I'll use an example from natural language processing. Imagine you want to know the probability of this sentence:\n",
    "\n",
    "> I enjoy coffee\n",
    "\n",
    "In a Markov model, you could estimate its probability by calculating:\n",
    "\n",
    "$ P(WORD = I) * P(WORD = enjoy | PREVIOUS-WORD = I) * P(WORD = coffee| PREVIOUS-WORD = enjoy)$ \n",
    "\n",
    "Now, imagine we wanted to know the parts-of-speech tags of this sentence, that is, if a word is a past tense verb, a noun, etc. We did not observe any parts-of-speech tags in that sentence, but we assume they are there. Thus, we calculate what's the probability of the parts-of-speech tag sequence. In our case, the actual sequence is:\n",
    "\n",
    "PRP-VBP-NN [https://cs.nyu.edu/grishman/jet/guide/PennPOS.html]\n",
    "\n",
    "Since the parts-of-speech sequence are never directly observed, this can be considered as hidden states: we'd like to find the hidden sequence that best explains our observation. [https://stackoverflow.com/questions/10748426/what-is-the-difference-between-markov-chains-and-hidden-markov-model]\n",
    "\n",
    "A HMM model is specified by the following components: \n",
    "\n",
    "\n",
    "* A **set of N states** described by $Q$: $ Q = q_1, q_2…q_N$ the state at time $t$ is $q_t$\n",
    "\n",
    "* A **transition probability matrix A**:\n",
    "  * A set of probabilities $$A = a_{01}a_{02}…a_{n1}…a_{nn}$$\n",
    "  * Each $a_{ij}$ represents the probability of transitioning from state i to state j\n",
    "  * The\tset\tof\tthese\tis\tthe\ttransition\tprobability\tmatrix\tA \n",
    "  \n",
    "  $$a_{ij} = P(q_t = j | q_{t−1} = i) 1≤ i, j ≤ N $$\n",
    "  $$\\sum_{j=1}^n a_{ij} = 1 ∀i$$\n",
    "  \n",
    "* A sequence of $T$ observations $O = o_1 o_2 ...o_T$, each one drawn from a vocabulary $V=v_1, v_2,...,v_V$\n",
    "\n",
    "* A sequence of observations likelihoods, also called emission probabilities, each expressing the probability of an observation being generated from a state i $B = b_i(o_t)$\n",
    "\n",
    "* A special start state and end (final) state that are not associated with observations, together with transition probabilities $a_{01}a_{02} ...a_{0n}$ out of the start state and $a_{1F}a_{2F} ...a_{nF}$ into the end state.\n",
    "\n",
    "\n",
    "##### HMM Assumptions\n",
    "\n",
    "* **The probability of a particular state depends only on the previous state:**\n",
    "\n",
    "  $$ P(q_i|q_1...q_{i−1}) = P(q_i|q_{i−1})$$\n",
    "  \n",
    "\n",
    "*  **Output Independence:** The probability of an output observation oi depends only on the state that produced the observation qi and not on any other states or any other observations. Thus, for Hidden Markov models, each hidden state produces only a single observation:\n",
    "\n",
    "\n",
    "$$P(o_i|q_1 ...q_i,...,q_T , o_1,...,o_i,...,o_T ) = P(o_i|q_i)$$\n",
    "    \n",
    "#### Types of HMM's\n",
    "\n",
    "Bakis HMMs are generally used to model temporal processes like speech.\n",
    "\n",
    "<img src=\"assets/bakis-ergodic-hmm.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Three Basic Problems we can address with HMMs\n",
    "\n",
    "Now that we have seen the structure of an HMM, we turn to algorithms for computing things with them. An influential tutorial by [Rabiner (1989)](http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf), based on tutorials by Jack Ferguson in the 1960s, introduced the idea that **hidden Markov models should be characterized by three fundamental problems:**\n",
    "\n",
    "* Problem 1 (Likelihood or evaluation): the\tobservation\tsequence $O=(o_1 o_2 … o_T)$, and an HMM model $ \\phi = (A,B)$, how do we efficiently compute $P(O|\\phi)$, the probability of the observation sequence, given the model?\n",
    "\n",
    "\n",
    "\n",
    "* Problem 2 (Decoding): Given\tthe\tobservation\tsequence $O=(o_1 o_2 … o_T)$, and an HMM model $\\phi = (A,B)$, how do we\tchoose\ta\tcorresponding state sequence $Q=(q_1 q_2 … q_T)$ that\tis\toptimal\tin\tsome sense?\t-e.g. the seq of hidden states that better explains the observations-\n",
    "\n",
    "\n",
    "\n",
    "* Problem 3 (Learning): How\tdo we adjust the model parameters $\\phi =(A,B)$ to\tmaximize $P(O|\\phi)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
